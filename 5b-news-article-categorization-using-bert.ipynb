{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":2556316,"sourceType":"datasetVersion","datasetId":1550907}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"https://www.kaggle.com/datasets/timilsinabimal/newsarticlecategories","metadata":{}},{"cell_type":"code","source":"# 5b\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom transformers import BertTokenizer, BertForSequenceClassification, AdamW\nfrom torch.utils.data import DataLoader, TensorDataset\nimport torch\nfrom sklearn.metrics import accuracy_score, classification_report\n\n# Load the dataset with 'category', 'title', and 'body' columns\ndf = pd.read_csv('/kaggle/input/newsarticlecategories/news-article-categories.csv')\n\n# Convert categories to numerical labels\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\ndf['category'] = le.fit_transform(df['category'])\n\n# Ensure the 'body' column contains strings (in case of NaNs or other issues)\ndf['body'] = df['body'].astype(str)\n\n# Split the dataset into training and testing sets\ntrain_texts, val_texts, train_labels, val_labels = train_test_split(df['body'], df['category'], test_size=0.2, random_state=42)\n\n# Load BERT tokenizer\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n\n# Tokenize and encode sequences\nmax_length = 128  # Maximum token length\ntrain_encodings = tokenizer(train_texts.tolist(), truncation=True, padding=True, max_length=max_length)\nval_encodings = tokenizer(val_texts.tolist(), truncation=True, padding=True, max_length=max_length)\n\n# Convert labels to tensors\ntrain_labels = torch.tensor(train_labels.tolist())\nval_labels = torch.tensor(val_labels.tolist())\n\n# Convert encodings to tensors\ntrain_input_ids = torch.tensor(train_encodings['input_ids'])\ntrain_attention_masks = torch.tensor(train_encodings['attention_mask'])\n\nval_input_ids = torch.tensor(val_encodings['input_ids'])\nval_attention_masks = torch.tensor(val_encodings['attention_mask'])\n\n# Create PyTorch dataset\ntrain_dataset = TensorDataset(train_input_ids, train_attention_masks, train_labels)\nval_dataset = TensorDataset(val_input_ids, val_attention_masks, val_labels)\n\n# Create DataLoader for efficient batch processing\ntrain_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n\n# Load pre-trained BERT model for sequence classification\nmodel = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(df['category'].unique()))\nmodel = model.to('cuda')  # If using a GPU\n\n# Optimizer and learning rate scheduler\noptimizer = AdamW(model.parameters(), lr=1e-5)\n\n# Training Loop\nepochs = 3\nfor epoch in range(epochs):\n    model.train()\n    total_loss = 0\n\n    for batch in train_loader:\n        input_ids, attention_masks, labels = [b.to('cuda') for b in batch]\n        \n        optimizer.zero_grad()\n        \n        outputs = model(input_ids, attention_mask=attention_masks, labels=labels)\n        loss = outputs.loss\n        total_loss += loss.item()\n        \n        loss.backward()\n        optimizer.step()\n\n    avg_train_loss = total_loss / len(train_loader)\n    print(f'Epoch {epoch + 1}, Loss: {avg_train_loss}')\n\n# Evaluation\nmodel.eval()\npredictions, true_labels = [], []\n\nwith torch.no_grad():\n    for batch in val_loader:\n        input_ids, attention_masks, labels = [b.to('cuda') for b in batch]\n        \n        outputs = model(input_ids, attention_mask=attention_masks)\n        logits = outputs.logits\n        preds = torch.argmax(logits, dim=1)\n        \n        predictions.extend(preds.cpu().numpy())\n        true_labels.extend(labels.cpu().numpy())\n\n# Calculate accuracy and other metrics\naccuracy = accuracy_score(true_labels, predictions)\nprint(f'Validation Accuracy: {accuracy}')\nprint(classification_report(true_labels, predictions, target_names=le.classes_))\n\n# Sample input and prediction\nsample_text = \"Researchers have discovered a new species of butterfly in the Amazon rainforest.\"\nencoded_sample = tokenizer(sample_text, return_tensors='pt', truncation=True, padding=True, max_length=max_length)\nencoded_sample = {key: value.to('cuda') for key, value in encoded_sample.items()}\n\n# Predict category\nmodel.eval()\nwith torch.no_grad():\n    output = model(**encoded_sample)\n    predicted_class = torch.argmax(output.logits, dim=1).item()\n\npredicted_category = le.inverse_transform([predicted_class])\nprint(f'Predicted Category: {predicted_category[0]}')\n","metadata":{"execution":{"iopub.status.busy":"2024-10-18T17:24:16.400536Z","iopub.execute_input":"2024-10-18T17:24:16.401343Z","iopub.status.idle":"2024-10-18T17:31:00.047510Z","shell.execute_reply.started":"2024-10-18T17:24:16.401286Z","shell.execute_reply":"2024-10-18T17:31:00.046329Z"},"trusted":true},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cceaed4ff6fd40a59eb1db2aea15a7b3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5b081eb9ee84417bbe708762df048a9d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"13bc87c09a07486d8b59ac387380a2e7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e5817bf9b0ff49019e3c2a924e0a257e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"12c9e56b1b6a4b8da21bcf0927a6d214"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1, Loss: 1.8205220279998557\nEpoch 2, Loss: 0.8643427667922752\nEpoch 3, Loss: 0.5662715161262557\nValidation Accuracy: 0.7776162790697675\n                precision    recall  f1-score   support\n\nARTS & CULTURE       0.84      0.89      0.87       206\n      BUSINESS       0.77      0.65      0.70       116\n        COMEDY       0.81      0.74      0.77        73\n         CRIME       0.72      0.78      0.75        54\n     EDUCATION       0.86      0.77      0.81       101\n ENTERTAINMENT       0.77      0.80      0.79        99\n   ENVIRONMENT       0.89      0.76      0.82       110\n         MEDIA       0.73      0.71      0.72        63\n      POLITICS       0.78      0.66      0.71       102\n      RELIGION       0.77      0.85      0.81        93\n       SCIENCE       0.69      0.83      0.75        58\n        SPORTS       0.95      0.84      0.89       100\n          TECH       0.68      0.81      0.74        95\n         WOMEN       0.58      0.70      0.64       106\n\n      accuracy                           0.78      1376\n     macro avg       0.78      0.77      0.77      1376\n  weighted avg       0.79      0.78      0.78      1376\n\nPredicted Category: ENVIRONMENT\n","output_type":"stream"}]}]}