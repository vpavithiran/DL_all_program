{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.15","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"tpu1vmV38","dataSources":[{"sourceId":320111,"sourceType":"datasetVersion","datasetId":134715}],"dockerImageVersionId":30788,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"https://www.kaggle.com/datasets/mdismielhossenabir/sentiment-analysis","metadata":{}},{"cell_type":"code","source":"# 6b\nimport torch\nfrom transformers import BertTokenizer, BertForSequenceClassification\nfrom transformers import AdamW\nfrom torch.utils.data import DataLoader, Dataset\nfrom sklearn.metrics import accuracy_score, classification_report\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\nfrom tqdm import tqdm\n\n# Check for GPU\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# Load IMDb dataset (you can download the dataset from https://ai.stanford.edu/~amaas/data/sentiment/)\n# Assuming 'imdb_dataset.csv' with columns 'review' and 'sentiment' (positive=1, negative=0)\ndf = pd.read_csv('/kaggle/input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv')\ndf['sentiment'] = df['sentiment'].map({'positive': 1, 'negative': 0})\n\n# Split the dataset\nX_train, X_test, y_train, y_test = train_test_split(df['review'], df['sentiment'], test_size=0.2, random_state=42)\n\n# Define a custom dataset class\nclass IMDbDataset(Dataset):\n    def __init__(self, reviews, sentiments, tokenizer, max_len):\n        self.reviews = reviews\n        self.sentiments = sentiments\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n    \n    def __len__(self):\n        return len(self.reviews)\n    \n    def __getitem__(self, idx):\n        review = self.reviews.iloc[idx]\n        sentiment = self.sentiments.iloc[idx]\n        encoding = self.tokenizer.encode_plus(\n            review,\n            truncation=True,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            return_token_type_ids=False,\n            padding='max_length',\n            return_attention_mask=True,\n            return_tensors='pt'\n        )\n        \n        return {\n            'review_text': review,\n            'input_ids': encoding['input_ids'].flatten(),\n            'attention_mask': encoding['attention_mask'].flatten(),\n            'sentiment': torch.tensor(sentiment, dtype=torch.long)\n        }\n\n# Load pre-trained BERT tokenizer\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n\n# Create data loaders\nMAX_LEN = 128\nBATCH_SIZE = 16\n\ntrain_dataset = IMDbDataset(X_train, y_train, tokenizer, MAX_LEN)\ntest_dataset = IMDbDataset(X_test, y_test, tokenizer, MAX_LEN)\n\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n\n# Load pre-trained BERT model\nmodel = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\nmodel = model.to(device)\n\n# Optimizer and learning rate scheduler\noptimizer = AdamW(model.parameters(), lr=2e-5)\n\n# Training function\ndef train_epoch(model, data_loader, optimizer, device):\n    model.train()\n    total_loss = 0\n    correct_predictions = 0\n    \n    for batch in tqdm(data_loader):\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        sentiments = batch['sentiment'].to(device)\n        \n        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=sentiments)\n        loss = outputs.loss\n        logits = outputs.logits\n        \n        _, preds = torch.max(logits, dim=1)\n        correct_predictions += torch.sum(preds == sentiments)\n        total_loss += loss.item()\n        \n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n    \n    return correct_predictions.double() / len(data_loader.dataset), total_loss / len(data_loader)\n\n# Evaluation function\ndef evaluate_model(model, data_loader, device):\n    model.eval()\n    total_loss = 0\n    correct_predictions = 0\n    \n    with torch.no_grad():\n        for batch in data_loader:\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            sentiments = batch['sentiment'].to(device)\n            \n            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=sentiments)\n            loss = outputs.loss\n            logits = outputs.logits\n            \n            _, preds = torch.max(logits, dim=1)\n            correct_predictions += torch.sum(preds == sentiments)\n            total_loss += loss.item()\n    \n    return correct_predictions.double() / len(data_loader.dataset), total_loss / len(data_loader)\n\n# Training loop\nEPOCHS = 1\n\nfor epoch in range(EPOCHS):\n    print(f'Epoch {epoch + 1}/{EPOCHS}')\n    train_acc, train_loss = train_epoch(model, train_loader, optimizer, device)\n    print(f'Train loss {train_loss}, accuracy {train_acc}')\n    \n    test_acc, test_loss = evaluate_model(model, test_loader, device)\n    print(f'Test loss {test_loss}, accuracy {test_acc}')\n\n# Evaluate the model on test set\ny_pred = []\ny_true = []\n\nmodel.eval()\nwith torch.no_grad():\n    for batch in test_loader:\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        sentiments = batch['sentiment'].to(device)\n        \n        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n        logits = outputs.logits\n        _, preds = torch.max(logits, dim=1)\n        \n        y_pred.extend(preds.cpu().numpy())\n        y_true.extend(sentiments.cpu().numpy())\n\n# Print classification report\nprint(classification_report(y_true, y_pred, target_names=['negative', 'positive']))\n","metadata":{"execution":{"iopub.status.busy":"2024-10-18T17:57:00.386858Z","iopub.execute_input":"2024-10-18T17:57:00.387852Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}