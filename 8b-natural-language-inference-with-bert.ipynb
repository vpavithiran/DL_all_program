{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# 8b\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport random\n\n# Set the device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Sample Data (small conversation for demo)\npairs = [\n    (\"hi\", \"hello\"),\n    (\"how are you?\", \"i am fine\"),\n    (\"what is your name?\", \"i am a chatbot\"),\n    (\"bye\", \"goodbye\")\n]\n\n# Vocabulary setup\ndef tokenize(sentence):\n    return sentence.lower().split()\n\ndef build_vocab(pairs):\n    vocab = set()\n    for pair in pairs:\n        for sentence in pair:\n            vocab.update(tokenize(sentence))\n    word2idx = {word: idx for idx, word in enumerate(vocab, 1)}\n    word2idx[\"<pad>\"] = 0  # Padding token\n    word2idx[\"<eos>\"] = len(word2idx)  # EOS token\n    idx2word = {idx: word for word, idx in word2idx.items()}\n    return word2idx, idx2word\n\nword2idx, idx2word = build_vocab(pairs)\nvocab_size = len(word2idx)\n\n# Encode sentences to token IDs\ndef encode(sentence):\n    return [word2idx.get(word, 0) for word in tokenize(sentence)] + [word2idx[\"<eos>\"]]  # Add EOS at the end\n\ndef decode(indices):\n    return \" \".join([idx2word[idx] for idx in indices if idx != 0 and idx != word2idx[\"<eos>\"]])\n\n# Define Encoder Model\nclass Encoder(nn.Module):\n    def __init__(self, input_size, hidden_size):\n        super(Encoder, self).__init__()\n        self.embedding = nn.Embedding(input_size, hidden_size)\n        self.rnn = nn.GRU(hidden_size, hidden_size, batch_first=True)\n\n    def forward(self, x):\n        embedded = self.embedding(x)\n        output, hidden = self.rnn(embedded)\n        return hidden\n\n# Define Decoder Model\nclass Decoder(nn.Module):\n    def __init__(self, hidden_size, output_size):\n        super(Decoder, self).__init__()\n        self.embedding = nn.Embedding(output_size, hidden_size)\n        self.rnn = nn.GRU(hidden_size, hidden_size, batch_first=True)\n        self.fc = nn.Linear(hidden_size, output_size)\n\n    def forward(self, x, hidden):\n        embedded = self.embedding(x)\n        output, hidden = self.rnn(embedded, hidden)\n        prediction = self.fc(output)\n        return prediction, hidden\n\n# Hyperparameters\nhidden_size = 32\nencoder = Encoder(vocab_size, hidden_size).to(device)\ndecoder = Decoder(hidden_size, vocab_size).to(device)\n\n# Loss and optimizer\ncriterion = nn.CrossEntropyLoss(ignore_index=0)  # Ignore padding\noptimizer = optim.Adam(list(encoder.parameters()) + list(decoder.parameters()), lr=0.01)\n\n# Training Loop (for simplicity, we'll use only 100 epochs)\nfor epoch in range(100):\n    for input_sentence, target_sentence in pairs:\n        # Prepare input and target tensors\n        input_tensor = torch.tensor([encode(input_sentence)], dtype=torch.long).to(device)\n        target_tensor = torch.tensor([encode(target_sentence)], dtype=torch.long).to(device)\n\n        # Forward pass through encoder\n        encoder_hidden = encoder(input_tensor)\n\n        # Decoder input starts with <pad> token\n        decoder_input = torch.tensor([[0]], dtype=torch.long).to(device)\n        decoder_hidden = encoder_hidden\n        loss = 0\n        for t in range(target_tensor.size(1)):\n            output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n            loss += criterion(output.squeeze(1), target_tensor[:, t])\n\n            # Teacher forcing: use target as next input\n            decoder_input = target_tensor[:, t].unsqueeze(1)\n\n        # Backpropagation\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n    if epoch % 10 == 0:\n        print(f'Epoch {epoch}, Loss: {loss.item():.4f}')\n\n# Inference: Chat with the chatbot\ndef chat(input_text):\n    input_tensor = torch.tensor([encode(input_text)], dtype=torch.long).to(device)\n    encoder_hidden = encoder(input_tensor)\n    decoder_input = torch.tensor([[0]], dtype=torch.long).to(device)  # Start token\n    decoder_hidden = encoder_hidden\n    output_sentence = []\n    for _ in range(10):  # Limit the max response length\n        output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n        top1 = output.argmax(2).item()\n        if top1 == word2idx[\"<eos>\"]:  # Stop if <eos> token is generated\n            break\n        output_sentence.append(top1)\n        decoder_input = torch.tensor([[top1]], dtype=torch.long).to(device)\n    return decode(output_sentence)\n\n# Chatting with the chatbot\nwhile True:\n    user_input = input(\"You: \")\n    if user_input.lower() in [\"bye\", \"exit\"]:\n        print(\"Chatbot: Goodbye!\")\n        break\n    response = chat(user_input)\n    print(f\"Chatbot: {response}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-10-18T18:46:31.448548Z","iopub.execute_input":"2024-10-18T18:46:31.449194Z","iopub.status.idle":"2024-10-18T18:47:39.446617Z","shell.execute_reply.started":"2024-10-18T18:46:31.449153Z","shell.execute_reply":"2024-10-18T18:47:39.445561Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Epoch 0, Loss: 5.9981\nEpoch 10, Loss: 0.3876\nEpoch 20, Loss: 0.0687\nEpoch 30, Loss: 0.0335\nEpoch 40, Loss: 0.0210\nEpoch 50, Loss: 0.0144\nEpoch 60, Loss: 0.0105\nEpoch 70, Loss: 0.0082\nEpoch 80, Loss: 0.0066\nEpoch 90, Loss: 0.0055\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"You:  hello\n"},{"name":"stdout","text":"Chatbot: goodbye\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"You:  hi\n"},{"name":"stdout","text":"Chatbot: hello\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"You:  iwhat is your name\n"},{"name":"stdout","text":"Chatbot: i am a chatbot\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"You:  bye\n"},{"name":"stdout","text":"Chatbot: Goodbye!\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}